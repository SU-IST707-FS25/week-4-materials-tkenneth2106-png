# Assignment Feedback: Week 4: Dimensionality Reduction

**Student:** tkenneth2106-png
**Total Score:** 19/40 (47.5%)

**Grade Category:** F (Failing)

---

## Problem Breakdown

### Exercise 1 (6/16 = 37.5%)

**Part pipeline-part1** (pipeline-part1.code): 0/0 points

_Feedback:_ Good work: you applied PCA to MNIST and visualized the 2D embedding. To better match “visualize the approximation,” also reconstruct images (pca.inverse_transform) and plot original vs reconstructed. Optionally standardize and explore variance explained or more components.

**Part pipeline-part2** (pipeline-part2.code): 1/4 points

_Feedback:_ You fit PCA and made a scree plot, but the task required a 2D scatter of the first two PCs colored by class. Missing: transform to 2 components and scatter plot with c=y_mnist_train. Reuse your prior PCA(n_components=2) and plot PC1 vs PC2 with label-based colors.

**Part pipeline-part3** (pipeline-part3.code): 1/4 points

_Feedback:_ You computed cumulative variance and the #components for 95%, which is related, but you didn’t create the required scree plot of the first 40 components with y-axis as percent variance explained. Plot explained_variance_ratio[:40]*100 vs components 1–40.

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ You correctly used n_components_95 from your prior work to select the components explaining 95% variance and applied it in PCA. While you didn’t recompute or print it here, leveraging the previously computed value is appropriate. Nice reconstruction demo.

**Part pipeline-part5** (pipeline-part5.code): 0/4 points

_Feedback:_ This cell doesn’t address the task. You were supposed to visualize a digit reconstructed from the reduced space using the dimensions from Step 4 (n_components_95), e.g., inverse_transform a sample and call plot_mnist_digit. Instead you trained KNN and printed accuracies.

---

### Exercise 2 (10/10 = 100.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Excellent. You correctly apply t-SNE to MNIST (sampling 2000 points is reasonable), set sensible parameters, and produce a clear 2D scatter with labels and colorbar. This fully meets the exercise goal.

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good work. You correctly used the t-SNE embedding (X_tsne from your prior work) with KNN, did a proper train/test split, trained KNN, and reported accuracy. This directly answers how it performs. Solid, appropriate approach.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Good job: you trained KNN on the UMAP features, predicted on a held-out split, and computed accuracy. Code is consistent with your prior UMAP work (X_umap/y_sample) and uses proper train/test split. Full credit.

---

### Exercise 4 (3/14 = 21.4%)

**Part ex2-part1** (ex2-part1.code): 0/0 points

_Feedback:_ Good start: you applied PCA (1D/2D), trained KNN, reported accuracies, and plotted 2D. Missing key parts: UMAP reductions, trying multiple UMAP params, comparing KNN performance across dimensions, and UMAP visualizations. Also consider tuning k and summarizing results.

**Part ex2-part2** (ex2-part2.code): 3/7 points

_Feedback:_ You implemented UMAP + KNN well with metrics and a clear plot, but the task was to try with PCA and leverage your prior PCA work. Please replace UMAP with PCA transforms (e.g., your PCA(2) pipeline), report accuracy, and plot the PCA projection to earn full credit.

**Part ex2-part3** (ex2-part3.answer): 0/7 points

_Feedback:_ Submission contains only placeholder text - no actual student work provided.

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-10-27 18:51:18 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*